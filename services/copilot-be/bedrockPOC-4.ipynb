{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7343603c-6ca0-4443-a316-67c6def0ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5805b5a94dcd4edda98dfe6566bb129b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search:', layout=Layout(width='70%'), placeholder='Search your documents')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ecaa214bd84e2e9d8ae1cccb40cca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Jurisdiction:', layout=Layout(width='50%'), options=('', 'Colorado', 'California', 'Texa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26119a9d50ee48a8ab4e3c3306e3d4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Doc Type:', layout=Layout(width='50%'), options=('', 'Guidance Memo', 'Regulation', 'Per…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2916de38aeee495eab245503ffa3472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search Documents', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e456983b97f41efa6b873b6a79ee721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 1: search, select\n",
    "import boto3\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Initialize the Kendra client\n",
    "kendra_client = boto3.client(\"kendra\")\n",
    "\n",
    "# Function to query the Kendra index with filters\n",
    "def query_kendra(index_id, query_text, jurisdiction=None, doc_type=None):\n",
    "    \"\"\"\n",
    "    Queries AWS Kendra with optional filters for jurisdiction and document type.\n",
    "    \"\"\"\n",
    "    # Build attribute filters dynamically\n",
    "    attribute_filters = []\n",
    "\n",
    "    if jurisdiction:\n",
    "        attribute_filters.append({\n",
    "            \"EqualsTo\": {\n",
    "                \"Key\": \"jurisdiction\",\n",
    "                \"Value\": {\"StringValue\": jurisdiction}\n",
    "            }\n",
    "        })\n",
    "\n",
    "    if doc_type:\n",
    "        attribute_filters.append({\n",
    "            \"EqualsTo\": {\n",
    "                \"Key\": \"doc_type\",\n",
    "                \"Value\": {\"StringValue\": doc_type}\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Combine filters if provided\n",
    "    attribute_filter = {\"AndAllFilters\": attribute_filters} if attribute_filters else None\n",
    "\n",
    "    # Query Kendra\n",
    "    response = kendra_client.query(\n",
    "        IndexId=index_id,\n",
    "        QueryText=query_text,\n",
    "        AttributeFilter=attribute_filter  # Apply filters if present\n",
    "    )\n",
    "\n",
    "    # Extract relevant results\n",
    "    return [\n",
    "        {\n",
    "            \"DocumentTitle\": item.get(\"DocumentTitle\", {}).get(\"Text\", \"No Title\"),\n",
    "            \"DocumentId\": item[\"DocumentId\"],\n",
    "            \"ExcerptText\": item.get(\"ExcerptText\", \"No Excerpt Available\")\n",
    "        }\n",
    "        for item in response[\"ResultItems\"] if \"DocumentId\" in item\n",
    "    ]\n",
    "\n",
    "# UI Elements for Document Search with Filters\n",
    "query_input = widgets.Text(\n",
    "    placeholder=\"Search your documents\",\n",
    "    description=\"Search:\",\n",
    "    layout=widgets.Layout(width=\"70%\")\n",
    ")\n",
    "\n",
    "jurisdiction_dropdown = widgets.Dropdown(\n",
    "    options=[\"\", \"Colorado\", \"California\", \"Texas\"],\n",
    "    description=\"Jurisdiction:\",\n",
    "    layout=widgets.Layout(width=\"50%\")\n",
    ")\n",
    "\n",
    "doc_type_dropdown = widgets.Dropdown(\n",
    "    options=[\"\", \"Guidance Memo\", \"Regulation\", \"Permit\"],\n",
    "    description=\"Doc Type:\",\n",
    "    layout=widgets.Layout(width=\"50%\")\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "select_button = widgets.Button(description=\"Select Documents\")\n",
    "\n",
    "# List to store selected document IDs and titles\n",
    "selected_document_ids = []\n",
    "selected_document_titles = []\n",
    "\n",
    "def on_search_click(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        results = query_kendra(\n",
    "            index_id=\"ac2e614a-1a60-4788-921f-439355c5756d\", \n",
    "            query_text=query_input.value,\n",
    "            jurisdiction=jurisdiction_dropdown.value.strip() or None,\n",
    "            doc_type=doc_type_dropdown.value.strip() or None\n",
    "        )\n",
    "        if not results:\n",
    "            display(Markdown(\"**No results found.**\"))\n",
    "            return\n",
    "\n",
    "        display(Markdown(\"**Search Results:**\"))\n",
    "        document_checkboxes = []\n",
    "        for result in results:\n",
    "            # Display result title and excerpt\n",
    "            display(Markdown(f\"**{result['DocumentTitle']}**\\n{result['ExcerptText']}\"))\n",
    "            # Append checkboxes with proper text\n",
    "            checkbox = widgets.Checkbox(description=result['DocumentTitle'], value=False)\n",
    "            checkbox.document_id = result[\"DocumentId\"]  # Store document ID in the checkbox\n",
    "            document_checkboxes.append(checkbox)\n",
    "\n",
    "        # Add the checkboxes and select button\n",
    "        select_button.on_click(lambda x: select_documents(document_checkboxes))\n",
    "        display(widgets.VBox(document_checkboxes))\n",
    "        display(select_button)\n",
    "\n",
    "def select_documents(document_checkboxes):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_document_ids.clear()\n",
    "        selected_document_titles.clear()\n",
    "        for checkbox in document_checkboxes:\n",
    "            if checkbox.value:  # If the checkbox is selected\n",
    "                selected_document_ids.append(checkbox.document_id)\n",
    "                selected_document_titles.append(checkbox.description)\n",
    "\n",
    "        if not selected_document_ids:\n",
    "            display(Markdown(\"**No documents selected.**\"))\n",
    "            return\n",
    "        \n",
    "        # Display the selected documents\n",
    "        display(Markdown(f\"**{len(selected_document_ids)} doc(s) selected. Proceed to step 2.**\"))\n",
    "        display(Markdown(f\"**Doc Titles:** {', '.join(selected_document_titles)}\"))\n",
    "\n",
    "# Display UI\n",
    "search_button = widgets.Button(description=\"Search Documents\")\n",
    "search_button.on_click(on_search_click)\n",
    "\n",
    "display(query_input, jurisdiction_dropdown, doc_type_dropdown, search_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508f6871-f9fa-412f-88aa-f2fd4bff87d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648a055d6d2b4e7e9a1dc57d7be8ba38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=500, description='Chunk Size:', max=2000, min=100, step=100), Button(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Fetch, Chunk and Map Chunk Source (new delete above if works)\n",
    "import boto3\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Function to fetch document text from S3\n",
    "def fetch_document_text(document_uri):\n",
    "    \"\"\"\n",
    "    Fetches document content from S3, dynamically handling multiple buckets\n",
    "    and supporting both PDFs and text files.\n",
    "    \"\"\"\n",
    "    # Extract bucket name and object key from S3 URI\n",
    "    match = re.match(r\"s3://([^/]+)/(.*)\", document_uri)  # Extract bucket and key\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid S3 URI format: {document_uri}\")\n",
    "\n",
    "    bucket_name, object_key = match.groups()\n",
    "    print(f\"Fetching from S3: Bucket={bucket_name}, Key={object_key}\")  # Debugging info\n",
    "\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        content = response[\"Body\"].read()\n",
    "\n",
    "        # Handle PDFs\n",
    "        if object_key.endswith(\".pdf\"):\n",
    "            pdf_reader = PdfReader(io.BytesIO(content))\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])\n",
    "            return text\n",
    "\n",
    "        return content.decode(\"utf-8\")  # Handle text files\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        raise ValueError(f\"Error: The file '{object_key}' does not exist in bucket '{bucket_name}'.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Unexpected error fetching document: {e}\")\n",
    "\n",
    "# Function to chunk document into smaller pieces\n",
    "def chunk_document(text, chunk_size=500):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# UI for Ingestion and Optimization\n",
    "chunk_size_slider = widgets.IntSlider(\n",
    "    value=500, min=100, max=2000, step=100, description=\"Chunk Size:\"\n",
    ")\n",
    "ingest_button = widgets.Button(description=\"Ingest Documents\")\n",
    "ingestion_output = widgets.Output()\n",
    "\n",
    "# Step 2: Store all document chunks properly\n",
    "all_chunks = []\n",
    "chunk_doc_map = []  # Stores mapping of chunk -> document title\n",
    "\n",
    "def ingest_documents(change):\n",
    "    global all_chunks, chunk_doc_map  # Store chunks and their source docs\n",
    "    with ingestion_output:\n",
    "        ingestion_output.clear_output()\n",
    "        if not selected_document_ids:\n",
    "            display(Markdown(\"**No documents selected. Please complete Step 1.**\"))\n",
    "            return\n",
    "\n",
    "        all_chunks = []  # Reset chunks\n",
    "        chunk_doc_map = []  # Reset mapping\n",
    "\n",
    "        for doc_id in selected_document_ids:\n",
    "            try:\n",
    "                doc_text = fetch_document_text(doc_id)\n",
    "                chunks = chunk_document(doc_text, chunk_size=chunk_size_slider.value)\n",
    "\n",
    "                # Store chunks and associate them with their document\n",
    "                all_chunks.extend(chunks)\n",
    "                chunk_doc_map.extend([doc_id] * len(chunks))  # Map each chunk to its document\n",
    "\n",
    "                display(Markdown(f\"Successfully processed document: **{doc_id}**\"))\n",
    "            except ValueError as e:\n",
    "                display(Markdown(f\"**Error:** {e}\"))\n",
    "\n",
    "        if all_chunks:\n",
    "            display(Markdown(f\"**{len(all_chunks)} document chunks stored. Proceed to Step 3.**\"))\n",
    "            \n",
    "            # Debugging: Print distinct document sources\n",
    "            unique_sources = set(chunk_doc_map)\n",
    "            print(\"Debugging: Unique document sources used:\")\n",
    "            for source in unique_sources:\n",
    "                print(source)\n",
    "\n",
    "        else:\n",
    "            display(Markdown(\"**No valid text extracted.**\"))\n",
    "\n",
    "ingest_button.on_click(ingest_documents)\n",
    "display(widgets.VBox([chunk_size_slider, ingest_button, ingestion_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7eaedf0-51ae-41f0-9897-964d7cb0707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging: Number of chunks available: 5031\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Show how many chunks available\n",
    "print(\"Debugging: Number of chunks available:\", len(all_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30adcdf-a0d8-4665-b841-426f0e7c57c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to end):  How do I report my emissions from well unloading in Colorado?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging: 35 relevant chunks selected.\n",
      "Debugging: Sources used: s3://sbx-kendra-index/Colorado/Colorado_-_Regulation_Number_3_-_Stationary_Source_Permitting_and_Air_Pollutant_Emission_Notice_Requirements.pdf, s3://sbx-kendra-index/Colorado/Colorado_-_Regulation_Number_7_-_Control_of_Emissions_from_Oil_and_Gas_Emissions_Operations.pdf, s3://sbx-colorado-only/Colorado/Colorado_-_General_Air_Permits__-_General_Permit_GP11.pdf, s3://sbx-colorado-only/Colorado/Colorado_-_Air_permitting_guidance_memos__-_Memo_20-04_-_Routine_or_Predictable_Gas_Venting_Emissions_Calculation_and_Instructions_on_Permitting_for_Oil_and_Natural_Gas_Operations.pdf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**User Question:** How do I report my emissions from well unloading in Colorado?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI Response:** Based on the regulations in Colorado Regulation Number 7, here are the key requirements for reporting emissions from well unloading:\n",
       "\n",
       "1. Owners/operators must submit a single annual report by June 30th each year that includes information on well unloading events from the previous calendar year.\n",
       "\n",
       "2. The report must include:\n",
       "\n",
       "- The API number of the well and AIRS number of any associated storage tanks\n",
       "\n",
       "- The date, time, and duration of each well unloading event\n",
       "\n",
       "- Whether the event was controlled or not (starting with 2023 reporting year)\n",
       "\n",
       "- The best management practices used to minimize emissions\n",
       "\n",
       "- Any safety needs that prevented using best practices\n",
       "\n",
       "- An estimate of the volume of natural gas, VOC, NOx, N2O, CO2, CO, ethane, and methane emitted during the event\n",
       "\n",
       "- The emission factor or calculation methodology used to determine emissions\n",
       "\n",
       "3. Records must be maintained for 5 years and made available to the Division upon request.\n",
       "\n",
       "4. The report must be submitted using a Division-approved format.\n",
       "\n",
       "5. A certification by a responsible official must accompany the report stating the information is true, accurate and complete.\n",
       "\n",
       "6. For wells in disproportionately impacted communities, additional reporting is required if there were 6 or more uncontrolled unloading events in any rolling 6-month period.\n",
       "\n",
       "7. Starting in 2023, the report must also indicate if the well is equipped with artificial lift.\n",
       "\n",
       "So in summary, detailed records of each unloading event must be kept and reported annually, including event details, emission estimates, and control information. The Division provides specific reporting forms and formats to use for submitting this information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Sources Used:** s3://sbx-kendra-index/Colorado/Colorado_-_Regulation_Number_3_-_Stationary_Source_Permitting_and_Air_Pollutant_Emission_Notice_Requirements.pdf, s3://sbx-kendra-index/Colorado/Colorado_-_Regulation_Number_7_-_Control_of_Emissions_from_Oil_and_Gas_Emissions_Operations.pdf, s3://sbx-colorado-only/Colorado/Colorado_-_General_Air_Permits__-_General_Permit_GP11.pdf, s3://sbx-colorado-only/Colorado/Colorado_-_Air_permitting_guidance_memos__-_Memo_20-04_-_Routine_or_Predictable_Gas_Venting_Emissions_Calculation_and_Instructions_on_Permitting_for_Oil_and_Natural_Gas_Operations.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to end):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "# Placeholder for document context from Step 2\n",
    "all_prompts = [\"Context: You are a helpful assistant that answers questions based on the provided context.\"]\n",
    "\n",
    "# Function to invoke the Claude model\n",
    "def invoke_claude_model(context, question, max_output_tokens=2000):\n",
    "    try:\n",
    "        # Build the messages list for Claude\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\nQuestion: {question}\"}\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            body=json.dumps({\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": max_output_tokens,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "        response_body = response[\"body\"].read().decode(\"utf-8\")\n",
    "        result = json.loads(response_body)\n",
    "\n",
    "        if \"content\" in result and isinstance(result[\"content\"], list):\n",
    "            return \" \".join([item[\"text\"] for item in result[\"content\"] if item[\"type\"] == \"text\"])\n",
    "        else:\n",
    "            raise ValueError(\"Claude API response does not contain valid 'content'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to find the most relevant chunks for a question\n",
    "def find_relevant_chunks(question, chunks, chunk_doc_map, num_chunks=15):\n",
    "    \"\"\"\n",
    "    Finds the most relevant chunks based on keyword overlap with the question and labels them with source documents.\n",
    "    \"\"\"\n",
    "    keywords = set(question.lower().split())  # Extract words from the user's question\n",
    "    scored_chunks = []\n",
    "    selected_sources = set()\n",
    "\n",
    "    # Score chunks based on keyword matches and retain document references\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        match_count = len(keywords.intersection(chunk_words))  # Count overlapping words\n",
    "        if match_count > 0:\n",
    "            doc_source = chunk_doc_map[i]  # Get original document source\n",
    "            scored_chunks.append((chunk, match_count, doc_source))  # Store chunk with doc source\n",
    "            selected_sources.add(doc_source)\n",
    "\n",
    "    # Sort by relevance (highest matches first)\n",
    "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top N relevant chunks\n",
    "    relevant_chunks = [f\"[Source: {chunk[2]}] {chunk[0]}\" for chunk in scored_chunks[:num_chunks]]\n",
    "\n",
    "    # Fallback: If no relevant chunks found, select evenly distributed chunks\n",
    "    if not relevant_chunks:\n",
    "        relevant_chunks = get_balanced_chunks(chunks, num_chunks)\n",
    "        selected_sources = set(chunk_doc_map[:num_chunks])\n",
    "\n",
    "    return relevant_chunks, list(selected_sources)\n",
    "\n",
    "# Chat with Documents\n",
    "def chat_with_documents():\n",
    "    global all_chunks, chunk_doc_map  \n",
    "\n",
    "    if not all_chunks:\n",
    "        print(\"Error: No document chunks available. Please run Step 2 first.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        question = input(\"Enter your question (or type 'exit' to end): \").strip()\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Exiting chat...\")\n",
    "            break\n",
    "\n",
    "        # Find the most relevant chunks for this question\n",
    "        context_chunks, selected_sources = find_relevant_chunks(question, all_chunks, chunk_doc_map, num_chunks=35)\n",
    "        context = \" \".join(context_chunks)\n",
    "\n",
    "        print(f\"Debugging: {len(context_chunks)} relevant chunks selected.\")\n",
    "        print(f\"Debugging: Sources used: {', '.join(selected_sources)}\")\n",
    "\n",
    "        response = invoke_claude_model(context, question, max_output_tokens=2000)\n",
    "\n",
    "        if response:\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(f\"**AI Response:** {response}\"))\n",
    "            display(Markdown(f\"**Sources Used:** {', '.join(selected_sources)}\"))\n",
    "        else:\n",
    "            print(\"No AI response received.\")\n",
    "\n",
    "# Start the interactive chat\n",
    "chat_with_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ec65a-9ea0-4244-b080-77fdb13d549f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
